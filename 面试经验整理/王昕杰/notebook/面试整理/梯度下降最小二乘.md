# 梯度下降 最小二乘法 牛顿法 推导

## 补充知识
泰勒展开：
如果两个连续的曲线想要一致，则他们在某一点的一阶导数，二，三...n阶导数应该相同。
现在用多项式g(x)去拟合f(x)，f(x) = g(x), 则应该在a这个点0-n阶导相同。

![20200606_182218_35](assets/20200606_182218_35.png)


## 线性/逻辑回归梯度下降
参考： https://www.cnblogs.com/pinard/p/5970503.html

线性回归前向传播 f(X) = XW + b
逻辑回归前向传播 f(X) = sigmoid(f(X)) = $ \frac{1}{1+e^{-XW}} $
其中Y,f(X)维度(m,1),X维度(m,n),W维度(n,1),b标量(1,),m个样本,n个特征
MSE损失函数的矩阵表达为 $1/2 (f(X)-Y)^T(f(X)-Y)$
对X求偏导为 $X^T (f(X)-Y), f(X) = XW + b $
逻辑回归损失函数矩阵表达为 $ -ylog(f(X)) - (1-y)log(1-f(X)) $
其中$f(X)=\frac{1}{1-e^{-XW}}$
对X求偏导为 $X^T (f(X)-Y)$
(如果有偏置项)对b求偏导为$(f(X)-Y)$

梯度更新流程:
1. 检查当前loss是否小于终止loss
2. 根据损失函数对权重矩阵a求偏导,得到梯度
3. 学习率*梯度,得到当前参数更新大小,并更新所有参数

```python
"""假设有m张图片, 展开后有n个特征向量, 逻辑归回"""
import numpy as np
def prepare_data(batch):
    X = np.zeros((m, n))
    for i in range(m):
        img = cv2.imread(paths[batch][i])
        img = norm(preprocess(img))
        X[i] = img.reshape(-1) # (n,)
    Y = np.ones((m, 1))
    return X, Y

def sigmoid(z):
    return 1 / (1 + np.exp(-z) + 1e-6)

def logit_loss(pred, target, m):
    loss = -target*np.log(pred) - (1-target)*np.log(1-pred)
    return np.sum(loss) / m

def mse_loss(pred, target):
    delta = pred - target
    return np.dot(delta.transpose(), delta) / 2

def forward(X, W):
    """ X (m,n) W (n,1) """
    return np.dot(X, W) + b

def backward(X, A, Y):
    delta = A - Y # (m, 1)
    dW = np.dot(X.transpose(), delta) / m # (n,1)
    db = np.sum(delta) / m                # (1,)
    return dW, db

W = np.random.rand(n, 1)
b = 0
end_loss = 1e-6
for i in range(iteration):
    X, Y = prepare_data(i) # X (m, n), Y (m, 1)
    A = forward(X, W, b) # (m, 1)
    A = sigmoid(A)
    loss = logit_loss(A, Y, m)
    if loss < end_loss:
        break
    dW, db = backward(X, A, Y, m) # (n, 1)
    W = W - lr * dW
    b = b - lr * db
```


- 梯度下降法和最小二乘法相比，梯度下降法需要选择步长，而最小二乘法不需要。梯度下降法是迭代求解，最小二乘法是计算解析解。如果样本量不算很大，且存在解析解，最小二乘法比起梯度下降法要有优势，计算速度很快。但是如果样本量很大，用最小二乘法由于需要求一个超级大的逆矩阵，这时就很难或者很慢才能求解解析解了，使用迭代的梯度下降法比较有优势。
- 梯度下降法和牛顿法/拟牛顿法相比，两者都是迭代求解，不过梯度下降法是梯度求解，而牛顿法/拟牛顿法是用二阶的海森矩阵的逆矩阵或伪逆矩阵求解。相对而言，使用牛顿法/拟牛顿法收敛更快。但是每次迭代的时间比梯度下降法长。

## 最小二乘法
假设使用MSE作为损失函数，m个样本，特征维度n
$$ L(a0,a1,...,an) = 1/(2m) \sum_{j=1}^m(f(x0^j,x1^j,...,xn^j) - y^j)^2 $$
f(X)为(m,1)向量，a为(n+1,1)向量，X为(m,n+1)维矩阵，多出来的1是常数项
$$ f(X) = Xa $$
$$ L = 1/2 (Xa-Y)^T(Xa-Y) $$
根据最小二乘法，对损失函数L对a向量求导取0
$$ \frac{\partial L(a)}{\partial a} = X^T(Xa-Y) = 0$$
整理可得
$$ a = (X^T X)^{-1} X^T Y $$

最小二乘法的局限性:
1. 最小二乘法需要计算$X^T X$的逆矩阵，有可能它的逆矩阵不存在，这样就没有办法直接用最小二乘法，此时梯度下降法仍然可以使用。可以通过对样本数据进行整理，去掉冗余特征。让$X^T X$的行列式不为0，然后继续使用最小二乘法。
2. 当样本特征n非常的大的时候，计算$X^T X$的逆矩阵是一个非常耗时的工作（nxn的矩阵求逆），甚至不可行。此时以梯度下降为代表的迭代法仍然可以使用。那这个n到底多大就不适合最小二乘法呢？如果你没有很多的分布式大数据计算资源，建议超过10000个特征就用迭代法。或者通过主成分分析降低特征的维度后再用最小二乘法。
3. 如果拟合函数不是线性的，这时无法使用最小二乘法，需要通过一些技巧转化为线性才能使用，此时梯度下降仍然可以用。
4. 当样本量m很少，小于特征数n的时候，这时拟合方程是欠定的，常用的优化方法都无法去拟合数据。当样本量m等于特征数n的时候，用方程组求解就可以了。当m大于n时，拟合方程是超定的，也就是我们常用与最小二乘法的场景了。

## 牛顿法
